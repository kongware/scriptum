## Combining Effects with Actions using Monad

### The limitation of applicative

In the chapter about applicatives we have learned that an applicative effect may depend on a previous effect but we cannot control an effect depending on a previous value. It turns out that despite this limitation applicative computations are applicable for a surprising number of scenarios. However, they are not always enough. We need a slightly less general, slightly more expressive alternative.

### Extending `Applicative` by `Monad`

Provided you have understood applicative functors it is only a minor step to comprehend monads. Their sole purpose is to overcome the limitation of applicatives. Before we take a close look into the `Monad` type class we will ascertain what they are not:

_Monads are no means to handle side effects!_

There is special type to do this. Monad and applicative are just two differently expressive ways to combine this type or types that handle any other computational effect.

#### From pure functions to actions

So far we lifted n-ary functions into the context of `n` applicative values. If the next effect should depend on the previous value we need more than a pure function. We need one that returns a value in an effectul context of the same type itself. This type of function is called action. An action is not impure, but it may represent an effectful computation, provided it is used as part of an applicative or monadic computation:

```javascript
// Functor

const arrMap = f => xs =>
  xs.map((x, i) => f(x, i));

// Applicative

const arrAp = tf => xs =>
  arrFold(acc => f =>
    arrAppend(acc)
      (arrMap(x => f(x)) (xs)))
        ([])
          (tf);

// auxiliary functions

const arrFold = f => init => xs => {
  let acc = init;
  
  for (let i = 0; i < xs.length; i++)
    acc = f(acc) (xs[i], i);

  return acc;
};

const arrAppend = xs => ys =>
  (xs.push.apply(xs, ys), xs);

const comp = f => g => x => f(g(x));

// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// result of the action interpreted as a value
arrAppend(
  foo(1) (2)) 
    (foo(3) (null)); // [1, 2, 3]

// result of the action interpreted as a computation
arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [2, 3], []]
```
[run code](https://repl.it/@scriptum/TubbyUnconsciousVolume)

If we use `foo` in a normal context it return value is treated like an ordinary value. However, if we use it as part of an applicative computation, the return value behaves differently. It now represents a computation, namely a non-deterministic computation in the example above. Since `foo` is an action that returns a value wrapped in an effectful context the next effect can depend on the previous value. But now we are stuck with a nested context. The desired result is `[1, 3, 2, 3]`. Obviously actions are only half the solution.

#### The `join` operation

Let us just join to effectful contexts:

```javascript
const arrEmpty = () => [];

// Monad

const arrJoin = xs =>
  arrFold(arrAppend)
    (arrEmpty())
      (xs);

// MAIN

arrJoin(
  arrAp(
    arrMap(x => y =>
      y === null
        ? []
        : [x, y])
            ([1, 2]))
              ([3, null])); // [1, 3, 2, 3]
```
[run code](https://repl.it/@scriptum/FragrantSugaryObjectpool)

`join` is sufficient and yields the expected result. Composing `join`, `ap` and `map` is not that convenient. We can do better.

#### The `chain` operation

`chain` takes an effectful context from a previous functorial computation and an action, transforms the value(s) inside the context and returns a new context of the same type with the transformed value(s) inside:

```javascript
// Monad

const arrChain = mx => fm =>
  arrFold(acc => x =>
    arrAppend(acc) (fm(x))) ([]) (mx);

// MAIN

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    y === null
      ? []
      : [x, y])); // [1, 3, 2, 3]
```
[run code](https://repl.it/@scriptum/HarmoniousAdorableScript)

In other functional languages this combinator is called `flatmap`.

### Value/effect dependency

The special trait of monads is their ability to choose the next effect depending on a previous value. While this dependency is only optional, it presupposes that the next effect must depend on the previous one. You cannot have one dependency without the other. Let us illustrate this in a more schematic manner. Given is an effectul computation `F<A>` where `A` is the result value of a previous effectful computation. It applies:

```
F   is an effectul context
A   is a pure value
~   denotes an x-may-depend-on-y relation
-   denotes an x-depends-on-y relation
</> denotes the direction of a dependency

Applicative constitutes:
F <~ F

Monad constitutes:
F <- F
A <~ F
```
Compared to applicatives this is a limitation of the monadic concept. We can demonstrate it using asynchronous computations that are run in parallel or in sequence:

```javascript
// record constructor

const record = (type, o) =>
  (o[type.name || type] = type.name || type, o);

// PARALLEL

const Parallel = par => record(
  Parallel,
  thisify(o => {
    o.par = (res, rej) =>
      par(x => {
        o.par = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const parMap = f => tx =>
  Parallel((res, rej) =>
    tx.par(x => res(f(x)), rej));

// Applicative

const parAp = tf => tx =>
  Parallel((res, rej) =>
    parAnd(tf) (tx)
      .par(([f, x]) =>
         res(f(x)), rej));

const parOf = x => Parallel((res, rej) => res(x));

// TASK

const Task = task => record(
  Task,
  thisify(o => {
    o.task = (res, rej) =>
      task(x => {
        o.task = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const taskMap = f => tx =>
  Task((res, rej) =>
    tx.task(x => res(f(x)), rej));

// Applicative

const taskAp = tf => tx =>
  Task((res, rej) =>
     tf.task(f =>
       tx.task(x =>
         res(f(x)), rej), rej));

const taskOf = x => Task((res, rej) => res(x));

// Monad

const taskChain = mx => fm =>
  Task((res, rej) =>
    mx.task(x =>
      fm(x).task(res, rej), rej));

// auxiliary functions

const id = x => x;
const thisify = f => f({});
const comp = f => g => x => f(g(x));
const add = x => y => x + y;

const delayParallel = f => ms => x =>
  Parallel((res, rej) => setTimeout(comp(res) (f), ms, x));

const delayTask = f => ms => x =>
  Task((res, rej) => setTimeout(comp(res) (f), ms, x));

// MAIN

const mainParallel = parAp(parMap(add)
  (delayParallel(id) (1000) (2)))
    (delayParallel(id) (1000) (3));

const mainTask = taskChain(
  delayTask(id) (1000) (2)) (x =>
    taskChain(delayTask(id) (1000) (3))
      (y => taskOf(x + y)));

// parallel addition: 2 * 1000ms = 1000ms
mainParallel.par(console.log); // logs 5 after 1000ms

// sequential addition: 2 * 1000ms = 2000ms
mainTask.task(console.log); // logs 5 after 2000ms
```
[run code](https://repl.it/@scriptum/LimeBraveFlatassembler)

`Task` runs its effect in parallel whereas `Parallel` runs it in parallel. `Task` implements `Monad` and thus also `Applicative`. `Parallel` on the other hand only implements the latter, because a monad by design cannot run in parallel. You might wonder why I did declare `Parallel` in the first place instead of implementing `Task`'s `Applicative` instance with in parallel semantics. Having a monadic type whose `Applicative` behaves differently is considered bad practice. This is only a convention but a useful one and you should stick to it.

The `Parallel` based computation lifts the pure `add` function into an asynchronous context by composing `map` with `ap`. The `Task` based computation on the other hand forms a nested closure. There is no separate function to be lifted, but the nested computation itself forms the monadic action. The attentive reader may have already noticed that a monad is not necessary for the above task. I have just chose addition for both computations as the underlying operation to demonstrate their differences.

Although monadic computations usually run in sequence, there are a few exceptions. It might be a simplification but I think the notion of monads as effect sequencing operators is still helpful. Just keep in mind that the order effects are performed in is not always deterministic.

### Essentially monadic

The special characteristic of monads is their ability to choose the next effect depending on a previous value. But what are the consequences of this dependency for a single `Monad` instance? What property of a type is essentially monadic? This questions refer to the hard part of the monadic concept, because the answer wildly varies for each instance. We have already seen what what is essentially monadic for the asynchronous `Task` type. It cannot run in parallel. Let us examine other types in this regard. 

#### Array instance

The essentially monadic property of non-deterministic arrays is their ability to form different shapes dynamically, that is, depending on values which are determined at runtime. Since an array is a linear data type different shape means different lengths:

```javascript
// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// Apllicative

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, 4]); // [[1, 3], [1, 4], [2, 3], [2, 4]]

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [1, 4], []]

// Monadic

arrChain([1, 2]) (x =>
  arrChain([3, 4]) (y =>
    foo(x) (y))); // [1, 3, 1, 4, 2, 3, 2, 4]

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    foo(x) (y))); // [1, 3, 1, 4]
```
[run code](https://repl.it/@scriptum/QueasyFrenchAnalysts)

With applicative it does not matter what we do the length of the resulting arrays is determined at compile time and thus known upfront. In our example the length is always four.  The monadic computations on the other hand can yield arrays of different length depending on dynamic values which are only evaluated at runtime. Admittedly I only use static array literals in the abvoe example, mostly for the sake of simplicity. I could have used dynamic values though and it would still work.

#### Function instance

The essentially monadic effect aspect of the function instance also known as the `Reader` monad is a completely different one:

```javascript
TODO
```
[run code](https://repl.it/@scriptum/UnpleasantViolentProlog)

### Monads at the type level

* the reverse `chain` combinator is used for the sake of symetry
* `chain` is an application operator as well as `map`/`ap`

```
<   A, B>( f:   (x: A) => B ) => ( x:   A ) =>   B ; // function application
<F, A, B>( f:   (x: A) => B ) => (tx: F<A>) => F<B>; // functor lifting
<F, A, B>(tf: F<(x: A) => B>) => (tx: F<A>) => F<B>; // applicative lifting
<M, A, B>(ft: (x: A) => M<B>) => (tx: M<A>) => M<B>; // monadic binding
```
### Monadic laws

* left identity
* right identity
* associativity

### Action composition Ã  la Kleisli

* show implementation of compk, reverse pipek (kleisli composition)

The reason the `chain` and not `kleisli` is part of the `Monad` API is that the both operations, which are associated to `Functor` and `Applicative`, are function application rather than function composition.

### Abstracting from nested application

### Recursion within a monad

* explain the difference between liftA2 and liftM2 (as opposed to Haskell)
* illustrate that liftM2 based on CPS is still more readable than deeply nested monad bindings
* show implementation of liftM2 combinator family
* show implementation of compk3 and pipek3 combinator families
* example with `Task` instance
* monad recursion
* monads allow also allow composition of actions
ma `bind` (f >=> g) = (ma `bind` f) `bind` g              -- bind = (>>=)
                    = (`bind` g) . (`bind` f) $ ma 
                    = join . fmap g . join . fmap f $ ma
with flipped >>=:
((g <=< f) =<<)  =  (g =<<) . (f =<<)  =  join . (g <$>) . join . (f <$>)
