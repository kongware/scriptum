## Combining Effects with Actions using Monad

### The limitation of applicative

In the chapter about applicatives we have learned that the next applicative effect may depend on a previous one but we cannot determine the next effect depending on a previous value. It turns out that despite this limitation applicative computations are applicable for a surprising number of cases. They are not always enough though. We need a slightly less general, slightly more expressive alternative.

### Extending `Applicative` by `Monad`

Provided you have understood applicative functors it is only a minor step to comprehend monads. They have essentially to purposes:

* to overcome the limitation of applicatives (encode a previous-value-next-effect-dependency)
* allow to short circuit any monadic computation no matter if a specific instance includes such a semantics

Knowing what the monad algebraic structure is not meant for can also be helpful:

* they are no means to contain side effects

Purely functional languages ship with a very special type to handle side effects or I/O respectively and to connect the pure realm of the language with the real world. This type is baked into the language's core. Monads as well as applicatives are merely useful to combine computations of this type. In Javascript there is no special I/O type, that is to say we need to mimic it with the means of the language.

#### From pure functions to actions

So far we lifted n-ary functions into the context of `n` applicative values. If we want to achieve either of the monadic purposes we need more than lifting a pure function. Required is a function of the following kind:

* it must return a value wrapped in the same effectful context as the rest of the monadic computation itself
* it must be nested to enable short circuiting at every step of the monadic computation

Such a function is called an action. An action is not impure, but it may represent an effectful computation, provided it is used as part of an applicative or monadic computation:

```javascript
// Functor

const arrMap = f => xs =>
  xs.map((x, i) => f(x, i));

// Applicative

const arrAp = tf => xs =>
  arrFold(acc => f =>
    arrAppend(acc)
      (arrMap(x => f(x)) (xs)))
        ([])
          (tf);

// auxiliary functions

const arrFold = f => init => xs => {
  let acc = init;
  
  for (let i = 0; i < xs.length; i++)
    acc = f(acc) (xs[i], i);

  return acc;
};

const arrAppend = xs => ys =>
  (xs.push.apply(xs, ys), xs);

const comp = f => g => x => f(g(x));

// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// result of the action interpreted as a value
arrAppend(
  foo(1) (2)) 
    (foo(3) (null)); // [1, 2, 3]

// result of the action interpreted as a computation
arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [2, 3], []]
```
[run code](https://repl.it/@scriptum/TubbyUnconsciousVolume)

When we use `foo` in a non-functorial context its result is treated like an ordinary value. However, when we use it as part of an applicative computation, the result is interpreted differently. It now represents a computation, namely a non-deterministic one in the example above. Since `foo` is an action that returns a value wrapped in an effectful context the next effect can depend on the previous value. But now we are stuck with a nested context. The desired result is `[1, 3, 2, 3]`. Obviously actions are only half the solution.

#### The `join` operation
***
Let us just join to effectful contexts:

```javascript
const arrEmpty = () => [];

// Monad

const arrJoin = xs =>
  arrFold(arrAppend)
    (arrEmpty())
      (xs);

// MAIN

arrJoin(
  arrAp(
    arrMap(x => y =>
      y === null
        ? []
        : [x, y])
            ([1, 2]))
              ([3, null])); // [1, 3, 2, 3]
```
[run code](https://repl.it/@scriptum/FragrantSugaryObjectpool)

`join` is sufficient and yields the expected result. Composing `join`, `ap` and `map` is not that convenient. We can do better.

#### The `chain` operation

`chain` takes an effectful context from a previous functorial computation and an action, transforms the value(s) inside the context and returns a new context of the same type with the transformed value(s) inside:

```javascript
// Monad

const arrChain = mx => fm =>
  arrFold(acc => x =>
    arrAppend(acc) (fm(x))) ([]) (mx);

// MAIN

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    y === null
      ? []
      : [x, y])); // [1, 3, 2, 3]
```
[run code](https://repl.it/@scriptum/HarmoniousAdorableScript)

In other functional languages this combinator is called `flatmap`.

### Value/effect dependency

The special trait of monads is their ability to choose the next effect depending on a previous value. While this dependency is only optional, it presupposes that the next effect must depend on the previous one. You cannot have one dependency without the other. Let us illustrate this in a more schematic manner. Given is an effectul computation `F<A>` where `A` is the result value of a previous effectful computation. It applies:

```
F   is an effectul context
A   is a pure value
~   denotes an x-may-depend-on-y relation
-   denotes an x-depends-on-y relation
</> denotes the direction of a dependency

Applicative constitutes:
F <~ F

Monad constitutes:
F <- F
A <~ F
```
Compared to applicatives this is a limitation of the monadic concept. We can demonstrate it using asynchronous computations that are run in parallel or in sequence:

```javascript
// record constructor

const record = (type, o) =>
  (o[type.name || type] = type.name || type, o);

// PARALLEL

const Parallel = par => record(
  Parallel,
  thisify(o => {
    o.par = (res, rej) =>
      par(x => {
        o.par = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const parMap = f => tx =>
  Parallel((res, rej) =>
    tx.par(x => res(f(x)), rej));

// Applicative

const parAp = tf => tx =>
  Parallel((res, rej) =>
    parAnd(tf) (tx)
      .par(([f, x]) =>
         res(f(x)), rej));

const parOf = x => Parallel((res, rej) => res(x));

// TASK

const Task = task => record(
  Task,
  thisify(o => {
    o.task = (res, rej) =>
      task(x => {
        o.task = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const taskMap = f => tx =>
  Task((res, rej) =>
    tx.task(x => res(f(x)), rej));

// Applicative

const taskAp = tf => tx =>
  Task((res, rej) =>
     tf.task(f =>
       tx.task(x =>
         res(f(x)), rej), rej));

const taskOf = x => Task((res, rej) => res(x));

// Monad

const taskChain = mx => fm =>
  Task((res, rej) =>
    mx.task(x =>
      fm(x).task(res, rej), rej));

// auxiliary functions

const id = x => x;
const thisify = f => f({});
const comp = f => g => x => f(g(x));
const add = x => y => x + y;

const delayParallel = f => ms => x =>
  Parallel((res, rej) => setTimeout(comp(res) (f), ms, x));

const delayTask = f => ms => x =>
  Task((res, rej) => setTimeout(comp(res) (f), ms, x));

// MAIN

const mainParallel = parAp(parMap(add)
  (delayParallel(id) (1000) (2)))
    (delayParallel(id) (1000) (3));

const mainTask = taskChain(
  delayTask(id) (1000) (2)) (x =>
    taskChain(delayTask(id) (1000) (3))
      (y => taskOf(x + y)));

// parallel addition: 2 * 1000ms = 1000ms
mainParallel.par(console.log); // logs 5 after 1000ms

// sequential addition: 2 * 1000ms = 2000ms
mainTask.task(console.log); // logs 5 after 2000ms
```
[run code](https://repl.it/@scriptum/LimeBraveFlatassembler)

`Task` runs its effect in parallel whereas `Parallel` runs it in parallel. `Task` implements `Monad` and thus also `Applicative`. `Parallel` on the other hand only implements the latter, because a monad by design cannot run in parallel. You might wonder why I did declare `Parallel` in the first place instead of implementing `Task`'s `Applicative` instance with in parallel semantics. Having a monadic type whose `Applicative` behaves differently is considered bad practice. This is only a convention but a useful one and you should stick to it.

The `Parallel` based computation lifts the pure `add` function into an asynchronous context by composing `map` with `ap`. The `Task` based computation on the other hand forms a nested closure. There is no separate function to be lifted, but the nested computation itself forms the monadic action. The attentive reader may have already noticed that a monad is not necessary for the above task. I have just chose addition for both computations as the underlying operation to demonstrate their differences.

Although monadic computations usually run in sequence, there are a few exceptions. It might be a simplification but I think the notion of monads as effect sequencing operators is still helpful. Just keep in mind that the order effects are performed in is not always deterministic.

### Essentially monadic

* array example
* function/reader counter example
* other example?

The special characteristic of monads is their ability to choose the next effect depending on a previous value. But what are the consequences of this dependency in practice? How does a result value of a monadic computation affect the next effect? What constitutes an essentially monadic computation? There is no general answer to this questions, because the essentially monadic aspect of an effectful context wildy varies across instances. We have already seen that the essentially monadic aspect of the asynchronous context encoded with the `Task` type is the inability to run in parallel. Let us examine other effectulf contexts and their associated types in this regard. 

#### Array instance

The essentially monadic aspect of non-deterministic arrays is their ability to form different shapes depending on dynamic values, that is, values which are only determined at runtime. Since an array is a linear data type different shapes mean different lengths:

```javascript
// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// Apllicative

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, 4]); // [[1, 3], [1, 4], [2, 3], [2, 4]]

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [1, 4], []]

// Monadic

arrChain([1, 2]) (x =>
  arrChain([3, 4]) (y =>
    foo(x) (y))); // [1, 3, 1, 4, 2, 3, 2, 4]

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    foo(x) (y))); // [1, 3, 1, 4]
```
[run code](https://repl.it/@scriptum/QueasyFrenchAnalysts)

With applicatives it does not matter what we do the length of the resulting arrays is determined at compile time and thus known upfront. In our example each applicative computation yields four-element arrays. The monadic computations on the other hand can yield arrays of different length depending on dynamic values which are only evaluated at runtime. In the above example I use merely static array literals for the sake of simplicity, but I could have used dynamic arrays and it would still work.

#### Function instance

The essentially monadic aspect of computations that share a common read-only environment is the ability to call a function none at all, once or several times:

```javascript
// FUNCTION

// Applicative

const funAp = tf => tg => x =>
  tf(x) (tg(x));

// Monad

const funChain = mg => fm => x =>
  fm(mg(x)) (x);

// auxiliary functions

const log = x => console.log(x);

const myDiv = env => x => y => {
  const r = x / y;
  if (env.debug) log(r);
  return r;
};

const mySqr = env => x => {
  const r = x * x;
  if (env.debug) log(r);
  return r;
};

// MAIN

const mainA = funAp(
  funAp(env =>
    env.y === 0
      ? _ => _ => _ => Infinity
//      ^^^^^^^^^ redundant function application
      : f => g => x => f(g(x)) (env.y))
//                     ^^^^^^ no explicit env passing
        (myDiv))
          (mySqr);

const mainM = funChain(myDiv) (f => env =>
  env.y === 0
    ? _ => Infinity
    : funChain(mySqr) (g => env =>
        x => f(g(x)) (env.y)) (env));
//           ^^^^^^ no explicit env passing

mainA({debug: true, y: 4}) (6); // logs 36, 9 and yields 9
mainA({debug: true, y: 0}) (6); // logs nothing and yields Infinity

mainM({debug: true, y: 4}) (6); // logs 36, 9 and yields 9
mainM({debug: true, y: 0}) (6); // logs nothing and yields Infinity
```
[run code](https://repl.it/@scriptum/UnpleasantViolentProlog)

TODO

### The two purposes of constructor invocations

There are two distinct purposes of constructor invocations in functional programming. An invocation like `[1, 2, 3]`, which happens to be in literal form in this example, can be used for the purpose of

* obtaining a collection of values of type natural number
* obtaining a non-deterministic computation that may yield none, one or many results

The first bullet point interprets `[1, 2, 3]` as an ordinary value and the second one as a computation with a specific computational effect, namely non-determinism in the given example. Depending on what purpose we pick we get completely different results:

***
```javascript
const arrAppend = xs => ys =>
  (xs.push.apply(xs, ys), xs);

const arrMap = f => xs =>
  xs.map((x, i) => f(x, i));

const arrAp = tf => xs =>
  arrReduce((acc, x) =>
    acc.concat(
      arrMap(x => f(x)) (xs)))
        ([])
          (tf);

// value notion
arrAppend([1, 2]) ([3, 4]); // [1, 2, 3, 4]

// computation notion
arrAp(
  arrMap(x => y => [x, y])) // A
    ([1, 2]) ([3, 4]); // [[1, 3], [1, 4], [2, 3], [2, 4]]
```
With the first application we just append two collections, which yields the union of both collections. The second application, however, yields all combinations of two non-deterministic computations. Please note that while the entire composition represents a non-deterministic computation, the array returned by the lambda in line `A` is again treated as a value. Mind-bending, right?

An applicative or the underlying functor respectively turns an ordinary value into a computation with a specific effect. You can think of applicative functors as semantics machines or little embedded effect-specific languages.

### Monads at the type level

* the reverse `chain` combinator is used for the sake of symetry
* `chain` is an application operator as well as `map`/`ap`

```
<   A, B>( f:   (x: A) => B ) => ( x:   A ) =>   B ; // function application
<F, A, B>( f:   (x: A) => B ) => (tx: F<A>) => F<B>; // functor lifting
<F, A, B>(tf: F<(x: A) => B>) => (tx: F<A>) => F<B>; // applicative lifting
<M, A, B>(ft: (x: A) => M<B>) => (tx: M<A>) => M<B>; // monadic binding
```
### Monadic laws

* left identity
* right identity
* associativity

### Action composition Ã  la Kleisli

* show implementation of compk, reverse pipek (kleisli composition)

The reason the `chain` and not `kleisli` is part of the `Monad` API is that the both operations, which are associated to `Functor` and `Applicative`, are function application rather than function composition.

### Abstracting from nested application

### Recursion within a monad

* explain the difference between liftA2 and liftM2 (as opposed to Haskell)
* illustrate that liftM2 based on CPS is still more readable than deeply nested monad bindings
* show implementation of liftM2 combinator family
* show implementation of compk3 and pipek3 combinator families
* example with `Task` instance
* monad recursion
* monads allow also allow composition of actions
ma `bind` (f >=> g) = (ma `bind` f) `bind` g              -- bind = (>>=)
                    = (`bind` g) . (`bind` f) $ ma 
                    = join . fmap g . join . fmap f $ ma
with flipped >>=:
((g <=< f) =<<)  =  (g =<<) . (f =<<)  =  join . (g <$>) . join . (f <$>)
